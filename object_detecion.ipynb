{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object_detecion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZPavlo/ML_projects/blob/master/object_detecion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpRiV7BAMww2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZilPuZHYTFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = \"/content/gdrive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIUX2x5hC580",
        "colab_type": "text"
      },
      "source": [
        "In this project I am create deep model to solve the object detection task. \n",
        "\n",
        "Base: https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsLq6KSdmW9x",
        "colab_type": "text"
      },
      "source": [
        "# Dataset review\n",
        "\n",
        "The first step in desiging any model is to find training data. In this task I use VOC dataset for Object detection task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9rksI3EG4XY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label map\n",
        "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "              'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
        "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
        "label_map['background'] = 0\n",
        "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
        "\n",
        "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
        "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8', '#f58231', '#911eb4', '#46f0f0', '#f032e6',\n",
        "                   '#d2f53c', '#fabebe', '#008080', '#000080', '#aa6e28', '#fffac8', '#800000', '#aaffc3', '#808000',\n",
        "                   '#ffd8b1', '#e6beff', '#808080', '#FFFFFF']\n",
        "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TWa-emmdbFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.datasets.voc import VOCDetection\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as func\n",
        "\n",
        "ROOT_PATH = os.path.join(PATH, \"datasets\")\n",
        "\n",
        "train_dataset = VOCDetection(ROOT_PATH, image_set=\"train\", download=False)\n",
        "val_dataset = VOCDetection(ROOT_PATH, image_set=\"val\", download=False)\n",
        "print(\"train:\", len(train_dataset)) # 5717\n",
        "print(\"val:\", len(val_dataset)) # 5823"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSqisz_kd_01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gvdVTU4tCzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(samples):\n",
        "\n",
        "    images = []\n",
        "    sizes = []\n",
        "    boxes_batch = []\n",
        "    labels_batch = []\n",
        "    diffs_batch = []\n",
        "\n",
        "    for (img, target) in samples:\n",
        "        \n",
        "        annotation = target[\"annotation\"]\n",
        "        boxes_dict = annotation[\"object\"]\n",
        "\n",
        "        if isinstance(boxes_dict, dict):\n",
        "            boxes_dict = [boxes_dict]\n",
        "        size = annotation[\"size\"]\n",
        "        size = (int(size[\"depth\"]), int(size[\"height\"]), int(size[\"width\"]))\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        difficult = []\n",
        "        for box in boxes_dict:\n",
        "            boxes.append([float(box[\"bndbox\"][\"xmin\"]), float(box[\"bndbox\"][\"ymin\"]),\n",
        "                          float(box[\"bndbox\"][\"xmax\"]), float(box[\"bndbox\"][\"ymax\"])])\n",
        "            labels.append(label_map[box[\"name\"]])\n",
        "            difficult.append(int(box[\"difficult\"]))\n",
        "\n",
        "        boxes = torch.FloatTensor(boxes)\n",
        "        \n",
        "        labels = torch.LongTensor(labels)\n",
        "        difficult = torch.LongTensor(difficult)\n",
        "\n",
        "        images.append(img)\n",
        "        boxes_batch.append(boxes)\n",
        "        sizes.append(size)\n",
        "        labels_batch.append(labels)\n",
        "        diffs_batch.append(difficult)\n",
        "\n",
        "    return {\"img\": images,\n",
        "            \"boxes\": boxes_batch,\n",
        "            \"size\": sizes,\n",
        "            \"labels\": labels_batch,\n",
        "            \"difficult\": diffs_batch\n",
        "            }\n",
        "\n",
        "train_load = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
        "                        num_workers=0, pin_memory=True, \n",
        "                        collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                        num_workers=0, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGMs6H-5E4dQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(train_load))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhS3Dqx22E2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize(image, boxes):\n",
        "    DIM = (300, 300)\n",
        "    boxes = boxes.clone()\n",
        "    w, h = image.size\n",
        "    image = func.resize(image, DIM)\n",
        "\n",
        "    boxes[:, 0] = boxes[:, 0] * (DIM[0] / float(w))\n",
        "    boxes[:, 1] = boxes[:, 1] * (DIM[1] / float(h))\n",
        "    boxes[:, 2] = boxes[:, 2] * (DIM[0] / float(w))\n",
        "    boxes[:, 3] = boxes[:, 3] * (DIM[1] / float(h))\n",
        "  \n",
        "    return image, boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8JE7l9Gesol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.utils as utils\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draph_boxes(image, objs, labels):\n",
        "  \n",
        "    image = np.array(image)\n",
        "\n",
        "    for (box, l) in zip(objs, labels):\n",
        "        l = int(l)\n",
        "        name = rev_label_map[l]\n",
        "        color = label_color_map[name].lstrip('#')\n",
        "        color = tuple(int(color[i:i+2], 16) for i in (0, 2, 4))\n",
        "        image = cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n",
        "        cv2.putText(image, name, (int(box[0]), int(box[1]) - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    return torch.from_numpy(np.transpose(image, (2, 0, 1)))\n",
        "\n",
        "new_imgs = []\n",
        "for i, img in enumerate(batch[\"img\"]):\n",
        "    new_im, boxes = resize(img, batch[\"boxes\"][i])\n",
        "    new_imgs.append(draph_boxes(new_im, boxes, batch[\"labels\"][i]))\n",
        "\n",
        "grid = utils.make_grid(new_imgs, 4, padding=2)\n",
        "ngrid = grid.numpy()\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(np.transpose(ngrid, (1, 2, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltq3S4dVByf-",
        "colab_type": "text"
      },
      "source": [
        "Create different transformation function for object detection task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js7q9q5ohv7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flip(image, boxes):\n",
        "\n",
        "    # Flip image\n",
        "    # image = func.to_pil_image(image)\n",
        "    new_image = func.hflip(image.copy())\n",
        "\n",
        "    # Flip boxes\n",
        "    new_boxes = boxes.clone()\n",
        "    new_boxes[:, 0] = image.width - boxes[:, 0] - 1\n",
        "    new_boxes[:, 2] = image.width - boxes[:, 2] - 1\n",
        "    new_boxes = new_boxes[:, [2, 1, 0, 3]]\n",
        "\n",
        "    return new_image, new_boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSQgCFWT1K6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img = batch[\"img\"][0]\n",
        "boxes = batch[\"boxes\"][0]\n",
        "labels = batch[\"labels\"][0]\n",
        "difficult = batch[\"difficult\"][0]\n",
        "size = batch[\"size\"][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kug_JhDX4y5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draph_img(img, boxes, labels):\n",
        "    draw_img = draph_boxes(img, boxes, labels)\n",
        "    plt.figure(dpi=150)\n",
        "    plt.imshow(np.transpose(draw_img.numpy(), (1, 2, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LG5Y80UIV3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draph_img(img, boxes, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2DUrxdX1MGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_flip, boxes_flip = flip(img, boxes)\n",
        "\n",
        "draph_img(img_flip, boxes_flip, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dvIWM11IiE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def photometric_distort(image):\n",
        "  \n",
        "    new_image = image\n",
        "\n",
        "    distortions = [func.adjust_brightness,\n",
        "                   func.adjust_contrast,\n",
        "                   func.adjust_saturation,\n",
        "                   func.adjust_hue]\n",
        "\n",
        "    for d in distortions:\n",
        "        if random.random() < 0.5:\n",
        "            if d.__name__ is 'adjust_hue':                \n",
        "                adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n",
        "            else:              \n",
        "                adjust_factor = random.uniform(0.5, 1.5)\n",
        "            \n",
        "            new_image = d(new_image, adjust_factor)\n",
        "\n",
        "    return new_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4OQjzBZOtSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_dist = photometric_distort(img)\n",
        "print(type(img_dist))\n",
        "draph_img(img_dist, boxes, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF-FSpH3aGBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.from_numpy(np.array(img)).shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8dhhjTZDG1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_iou(set_1, set_2):\n",
        "    \"\"\"\n",
        "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
        "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
        "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
        "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Find intersections\n",
        "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
        "\n",
        "    # Find areas of each box in both sets\n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
        "\n",
        "    # Find the union\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
        "\n",
        "    return intersection / union  # (n1, n2)\n",
        "\n",
        "def find_intersection(set_1, set_2):\n",
        "    \"\"\"\n",
        "    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n",
        "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
        "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
        "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
        "    \"\"\"\n",
        "\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
        "\n",
        "def random_crop(image, boxes, labels, difficulties):\n",
        "    \"\"\"\n",
        "    Performs a random crop in the manner stated in the paper. Helps to learn to detect larger and partial objects.\n",
        "    Note that some objects may be cut out entirely.\n",
        "    Adapted from https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n",
        "    :param image: image, a tensor of dimensions (3, original_h, original_w)\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
        "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
        "    :return: cropped image, updated bounding box coordinates, updated labels, updated difficulties\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = boxes.clone()\n",
        "    labels = labels.clone()\n",
        "    difficulties = difficulties.clone()\n",
        "\n",
        "    original_h = image.shape[1]\n",
        "    original_w = image.shape[2]\n",
        "  \n",
        "    while True:\n",
        "        min_overlap = random.choice([0., .1, .3, .5, .7, .9, None])  # 'None' refers to no cropping\n",
        "\n",
        "        # If not cropping\n",
        "        if min_overlap is None:\n",
        "            return image, boxes, labels, difficulties\n",
        "\n",
        "        max_trials = 50\n",
        "        for _ in range(max_trials):\n",
        "          \n",
        "            min_scale = 0.3\n",
        "            scale_h = random.uniform(min_scale, 1)\n",
        "            scale_w = random.uniform(min_scale, 1)\n",
        "            new_h = int(scale_h * original_h)\n",
        "            new_w = int(scale_w * original_w)\n",
        "\n",
        "            # Aspect ratio has to be in [0.5, 2]\n",
        "            aspect_ratio = new_h / new_w\n",
        "            if not 0.5 < aspect_ratio < 2:\n",
        "                continue\n",
        "\n",
        "            # Crop coordinates (origin at top-left of image)\n",
        "            left = random.randint(0, original_w - new_w)\n",
        "            right = left + new_w\n",
        "            top = random.randint(0, original_h - new_h)\n",
        "            bottom = top + new_h\n",
        "            crop = torch.FloatTensor([left, top, right, bottom])  # (4)\n",
        "\n",
        "            # Calculate Jaccard overlap between the crop and the bounding boxes\n",
        "            overlap = find_iou(crop.unsqueeze(0), boxes) \n",
        "            overlap = overlap.squeeze(0)  # (n_objects)\n",
        "\n",
        "            # If not a single bounding box has a Jaccard overlap of greater than the minimum, try again\n",
        "            if overlap.max().item() < min_overlap:\n",
        "                continue\n",
        "\n",
        "            # Crop image\n",
        "\n",
        "            new_image = image[:, top:bottom, left:right]  # (3, new_h, new_w)\n",
        "\n",
        "            # Find centers of original bounding boxes\n",
        "            bb_centers = (boxes[:, :2] + boxes[:, 2:]) / 2.  # (n_objects, 2)\n",
        "\n",
        "            # Find bounding boxes whose centers are in the crop\n",
        "            centers_in_crop = (bb_centers[:, 0] > left) * (bb_centers[:, 0] < right) * (bb_centers[:, 1] > top) * (\n",
        "                    bb_centers[:, 1] < bottom)  # (n_objects), a Torch uInt8/Byte tensor, can be used as a boolean index\n",
        "\n",
        "            # If not a single bounding box has its center in the crop, try again\n",
        "            if not centers_in_crop.any():\n",
        "                continue\n",
        "\n",
        "            # Discard bounding boxes that don't meet this criterion\n",
        "            new_boxes = boxes[centers_in_crop, :]\n",
        "            new_labels = labels[centers_in_crop]\n",
        "            new_difficulties = difficulties[centers_in_crop]\n",
        "\n",
        "            # Calculate bounding boxes' new coordinates in the crop\n",
        "            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])  # crop[:2] is [left, top]\n",
        "            new_boxes[:, :2] -= crop[:2]\n",
        "            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:])  # crop[2:] is [right, bottom]\n",
        "            new_boxes[:, 2:] -= crop[:2]\n",
        "\n",
        "            return new_image, new_boxes, new_labels, new_difficulties"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzuQh5zQS6Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_crop = func.to_tensor(img)\n",
        "img_crop, boxes_crop, labels_crop, difficult_crop = random_crop(img_crop, boxes, labels, difficult)\n",
        "img_crop = func.to_pil_image(img_crop)\n",
        "\n",
        "draph_img(img_crop, boxes_crop, labels_crop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUEGWrxxdZnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(image, boxes, labels, difficulties, split):\n",
        "\n",
        "    assert split in {'TRAIN', 'TEST', 'VAL'}\n",
        "\n",
        "    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n",
        "    # see: https://pytorch.org/docs/stable/torchvision/models.html\n",
        "    MEAN = [0.485, 0.456, 0.406]\n",
        "    STD = [0.229, 0.224, 0.225]\n",
        "    DIM = (300, 300)\n",
        "\n",
        "    new_image = image\n",
        "    new_boxes = boxes.clone()\n",
        "    new_labels = labels.clone()\n",
        "    new_difficulties = difficulties.clone()\n",
        "\n",
        "    if split in ['TRAIN', 'VAL']:\n",
        "        new_image = photometric_distort(new_image)\n",
        "\n",
        "        # Convert PIL image to Torch tensor\n",
        "        new_image = func.to_tensor(new_image)\n",
        "\n",
        "        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n",
        "                                                                         new_difficulties)\n",
        "\n",
        "        # Convert Torch tensor to PIL image\n",
        "        new_image = func.to_pil_image(new_image)\n",
        "\n",
        "        # Flip image with a 50% chance\n",
        "        if random.random() < 0.5:\n",
        "            new_image, new_boxes = flip(new_image, new_boxes)\n",
        "\n",
        "    w, h = new_image.size\n",
        "    new_image = func.resize(new_image, DIM)\n",
        "\n",
        "    new_boxes[:, 0] = new_boxes[:, 0] * (DIM[0] / float(w))\n",
        "    new_boxes[:, 1] = new_boxes[:, 1] * (DIM[1] / float(h))\n",
        "    new_boxes[:, 2] = new_boxes[:, 2] * (DIM[0] / float(w))\n",
        "    new_boxes[:, 3] = new_boxes[:, 3] * (DIM[1] / float(h))\n",
        "\n",
        "    new_image = func.to_tensor(new_image)\n",
        "    # new_image = func.normalize(new_image, mean=MEAN, std=STD)\n",
        "\n",
        "    return new_image, new_boxes, new_labels, new_difficulties"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJJe1v30gtO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_image, new_boxes, new_labels, new_difficulties = transform(img, boxes, labels, difficult, \"TRAIN\")\n",
        "\n",
        "print(new_image.shape)\n",
        "print(new_boxes.shape)\n",
        "print(new_labels.shape)\n",
        "print(new_difficulties.shape)\n",
        "\n",
        "d_img = func.to_pil_image(new_image)\n",
        "\n",
        "draph_img(d_img, new_boxes, new_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E483gC2I3FRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_fn(samples):\n",
        "\n",
        "    images = []\n",
        "    sizes = []\n",
        "    boxes_batch = []\n",
        "    labels_batch = []\n",
        "    diffs_batch = []\n",
        "\n",
        "    for (img, target) in samples:\n",
        "        \n",
        "        annotation = target[\"annotation\"]\n",
        "        boxes_dict = annotation[\"object\"]\n",
        "\n",
        "        if isinstance(boxes_dict, dict):\n",
        "            boxes_dict = [boxes_dict]\n",
        "        size = annotation[\"size\"]\n",
        "        size = (int(size[\"depth\"]), int(size[\"height\"]), int(size[\"width\"]))\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        difficult = []\n",
        "        for box in boxes_dict:\n",
        "            boxes.append([float(box[\"bndbox\"][\"xmin\"]), float(box[\"bndbox\"][\"ymin\"]),\n",
        "                          float(box[\"bndbox\"][\"xmax\"]), float(box[\"bndbox\"][\"ymax\"])])\n",
        "            labels.append(label_map[box[\"name\"]])\n",
        "            difficult.append(int(box[\"difficult\"]))\n",
        "\n",
        "        boxes = torch.FloatTensor(boxes)\n",
        "        \n",
        "        labels = torch.LongTensor(labels)\n",
        "        difficult = torch.LongTensor(difficult)\n",
        "\n",
        "        images.append(img)\n",
        "        boxes_batch.append(boxes)\n",
        "        sizes.append(size)\n",
        "        labels_batch.append(labels)\n",
        "        diffs_batch.append(difficult)\n",
        "\n",
        "    return {\"img\": images,\n",
        "            \"boxes\": boxes_batch,\n",
        "            \"size\": sizes,\n",
        "            \"labels\": labels_batch,\n",
        "            \"difficult\": diffs_batch\n",
        "            }\n",
        "\n",
        "\n",
        "train_dataset = VOCDetection(ROOT_PATH, image_set=\"train\", download=False)\n",
        "\n",
        "train_load_full = DataLoader(train_dataset, batch_size=8, shuffle=True, \n",
        "                        num_workers=2, pin_memory=True, \n",
        "                        collate_fn=collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQLnRFR98PIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Showing class imbalange\n",
        "num_classes = len(voc_labels) + 1\n",
        "\n",
        "# labels = torch.zeros(num_classes)\n",
        "# for batch in tqdm(train_load_full):\n",
        "#     for l in batch[\"labels\"]:\n",
        "#         labels +=  torch.bincount(l, minlength=num_classes)\n",
        "# print(labels)\n",
        "\n",
        "x = np.arange(labels.shape[0])\n",
        "plt.figure(dpi=100, figsize=(18, 3))\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "plt.bar(x, height=labels.numpy(), width=0.9)\n",
        "plt.xticks(x, [\"backgr\"] + list(voc_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4470HRQYCWgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(labels.shape[0])\n",
        "plt.figure(dpi=100, figsize=(18, 3))\n",
        "plt.rcParams.update({'font.size': 8})\n",
        "plt.bar(x, height=labels.numpy(), width=0.9)\n",
        "plt.xticks(x, [\"backgr\"] + list(voc_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDCPYOF40md0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "class MyVOCDetection(VOCDetection):\n",
        "\n",
        "    def __init__(self, root, download=False, transforms=None, split=\"TRAIN\"):\n",
        "      super(MyVOCDetection, self).__init__(root, image_set=split.lower(), download=download)\n",
        "      \n",
        "      self.my_transforms = transforms\n",
        "      self.split = split\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      img, target = super().__getitem__(index)\n",
        "\n",
        "      annotation = target[\"annotation\"]\n",
        "      boxes_dict = annotation[\"object\"]\n",
        "\n",
        "      if isinstance(boxes_dict, dict):\n",
        "        boxes_dict = [boxes_dict]\n",
        "      size = annotation[\"size\"]\n",
        "      size = (int(size[\"depth\"]), int(size[\"height\"]), int(size[\"width\"]))\n",
        "      \n",
        "      boxes = []\n",
        "      labels = []\n",
        "      difficult = []\n",
        "      for box in boxes_dict:\n",
        "          boxes.append([float(box[\"bndbox\"][\"xmin\"]), float(box[\"bndbox\"][\"ymin\"]),\n",
        "                        float(box[\"bndbox\"][\"xmax\"]), float(box[\"bndbox\"][\"ymax\"])])\n",
        "          labels.append(label_map[box[\"name\"]])\n",
        "          difficult.append(int(box[\"difficult\"]))\n",
        "\n",
        "      boxes = torch.FloatTensor(boxes)\n",
        "      labels = torch.LongTensor(labels)\n",
        "      difficult = torch.LongTensor(difficult)\n",
        "      \n",
        "      if self.my_transforms is not None:\n",
        "          img, boxes, labels, difficult = self.my_transforms(img, boxes, labels, difficult, self.split)\n",
        "\n",
        "      return (img, \n",
        "              size, \n",
        "              boxes, \n",
        "              labels,\n",
        "              difficult\n",
        "              )\n",
        "      \n",
        "    def collate_fn(self, samples):\n",
        "\n",
        "      images = []\n",
        "      sizes = []\n",
        "      boxes_batch = []\n",
        "      labels_batch = []\n",
        "      diffs_batch = []\n",
        "\n",
        "      for (img, size, boxes, lab, dif) in samples:\n",
        "          \n",
        "          images.append(img)\n",
        "          boxes_batch.append(boxes)\n",
        "          sizes.append(size)\n",
        "          labels_batch.append(lab)\n",
        "          diffs_batch.append(dif)\n",
        "\n",
        "      images = torch.stack(images)\n",
        "      \n",
        "      return {\"img\": images,\n",
        "              \"boxes\": boxes_batch,\n",
        "              \"size\": sizes,\n",
        "              \"labels\": labels_batch,\n",
        "              \"difficult\": diffs_batch\n",
        "              }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pURBaiTORNlq",
        "colab_type": "text"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-oW1ogtuZ_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def create_prior_boxes():\n",
        "\n",
        "    prior_boxes = []\n",
        "\n",
        "    for k, fmap in enumerate(fmaps):\n",
        "        f_size = fmap_dims[fmap]\n",
        "\n",
        "        cx = (np.arange(f_size) + 0.5) / f_size\n",
        "        cy = (np.arange(f_size) + 0.5) / f_size\n",
        "        x_grad, y_grad = np.meshgrid(cx, cy)\n",
        "\n",
        "        for ratio in aspect_ratios[fmap]:\n",
        "\n",
        "            w = np.ones_like(x_grad) * obj_scales[fmap] * math.sqrt(ratio)\n",
        "            h = np.ones_like(x_grad) * obj_scales[fmap] / math.sqrt(ratio)\n",
        "\n",
        "            anchors = np.stack([x_grad, y_grad, w, h], axis=-1).reshape(-1, 4)\n",
        "            prior_boxes.append(anchors)\n",
        "            \n",
        "            if ratio == 1.:\n",
        "                try:\n",
        "                    ad = math.sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
        "                except IndexError:\n",
        "                    ad = 1.\n",
        "                a = np.ones_like(x_grad) * ad\n",
        "                anchors = np.stack([x_grad, y_grad, a, a], axis=-1).reshape(-1, 4)\n",
        "                prior_boxes.append(anchors)\n",
        "\n",
        "    prior_boxes = np.concatenate(prior_boxes, axis=0).astype(\"float\")\n",
        "    return prior_boxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dlxXTbWHPLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models import vgg16\n",
        "import torch.nn as nn\n",
        "\n",
        "class VGG_Mode(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(VGG_Mode, self).__init__()\n",
        "\n",
        "        self.vgg = vgg16(pretrained=False)\n",
        "        # Change model\n",
        "        # 3-th MaxPool (add ceil_mode=True => 37x37 -> 38x38)\n",
        "        self.vgg.features[16] = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)\n",
        "        # 5-th MaxPool\n",
        "        self.vgg.features[30] = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.vgg.features1 = self.vgg.features[:23]\n",
        "        self.vgg.features2 = self.vgg.features[23:]\n",
        "        \n",
        "        # Replace FC -> Conv\n",
        "        self.vgg.classifier = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=1),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.vgg.features1(x)\n",
        "        out4_3 = x\n",
        "        x = self.vgg.features2(x)\n",
        "        out7 = self.classifier(x)\n",
        "\n",
        "        return out4_3, out7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhg6Ot9LMnDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AuxiliaryConvolutions(nn.Module):\n",
        "    \"\"\"\n",
        "    Additional convolutions to produce higher-level feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryConvolutions, self).__init__()\n",
        "\n",
        "        self.conv8 = nn.Sequential(nn.Conv2d(1024, 256, kernel_size=1, padding=0),\n",
        "                                   nn.ReLU(True),\n",
        "                                   nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "                                   nn.ReLU(True))\n",
        "\n",
        "        self.conv9 = nn.Sequential(nn.Conv2d(512, 128, kernel_size=1, padding=0),\n",
        "                                   nn.ReLU(True),\n",
        "                                   nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "                                   nn.ReLU(True))\n",
        "\n",
        "        self.conv10 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=1, padding=0),\n",
        "                                    nn.ReLU(True),\n",
        "                                    nn.Conv2d(128, 256, kernel_size=3, padding=0),\n",
        "                                    nn.ReLU(True))\n",
        "        \n",
        "        self.conv11 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=1, padding=0),\n",
        "                                    nn.ReLU(True),\n",
        "                                    nn.Conv2d(128, 256, kernel_size=3, padding=0),\n",
        "                                    nn.ReLU(True))\n",
        "  \n",
        "        self.init_conv2d()\n",
        "\n",
        "    def init_conv2d(self):\n",
        "        \"\"\"\n",
        "        Initialize convolution parameters.\n",
        "        \"\"\"\n",
        "        for seq in net.children():\n",
        "          for c in seq.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "\n",
        "    def forward(self, conv7_feats):\n",
        "\n",
        "        out = self.conv8(conv7_feats)\n",
        "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
        "\n",
        "        out = self.conv9(out)\n",
        "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
        "\n",
        "        out = self.conv10(out)\n",
        "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
        "\n",
        "        conv11_2_feats = self.conv11(out)# (N, 256, 1, 1)\n",
        "\n",
        "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzWiuFKZNHYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiBoxLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.priors_cxcy = priors_cxcy\n",
        "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
        "        self.threshold = threshold\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.smooth_l1 = nn.L1Loss()\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
        "\n",
        "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
        "\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "        n_classes = predicted_scores.size(2)\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).cuda()  # (N, 8732, 4)\n",
        "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).cuda()  # (N, 8732)\n",
        "\n",
        "        # For each image\n",
        "        for i in range(batch_size):\n",
        "            n_objects = boxes[i].size(0)\n",
        "\n",
        "            overlap = find_iou(boxes[i],\n",
        "                                           self.priors_xy)  # (n_objects, 8732)\n",
        "\n",
        "            # For each prior, find the object that has the maximum overlap\n",
        "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n",
        "\n",
        "            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n",
        "\n",
        "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).cuda()\n",
        "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
        "\n",
        "            # Labels for each prior\n",
        "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
        "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
        "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
        "\n",
        "            # Store\n",
        "            true_classes[i] = label_for_each_prior\n",
        "\n",
        "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
        "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
        "\n",
        "        positive_priors = true_classes != 0  # (N, 8732)\n",
        "\n",
        "        # LOCALIZATION LOSS\n",
        "\n",
        "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
        "\n",
        "        # CONFIDENCE LOSS\n",
        "\n",
        "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
        "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
        "\n",
        "        # First, find the loss for all priors\n",
        "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
        "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
        "\n",
        "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
        "\n",
        "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
        "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
        "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
        "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).cuda() # (N, 8732)\n",
        "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
        "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
        "\n",
        "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
        "\n",
        "        # TOTAL LOSS\n",
        "\n",
        "        return conf_loss + self.alpha * loc_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvQXUL3SMprg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSD300(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes, min_score, max_overlap, top_k):\n",
        "        super(SSD300, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.base = VGGBase()\n",
        "        self.aux_convs = AuxiliaryConvolutions()\n",
        "        self.pred_convs = PredictionConvolutions(n_classes)\n",
        "\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))\n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "        # Prior boxes\n",
        "        self.priors_cxcy = torch.from_numpy(create_prior_boxes()).cuda()\n",
        "\n",
        "        self.min_score = min_score\n",
        "        self.max_overlap = max_overlap\n",
        "        self.top_k = top_k\n",
        "\n",
        "\n",
        "    def forward(self, image, split):\n",
        "        \n",
        "        assert split in {'TRAIN', 'TEST', \"VAL}\n",
        "\n",
        "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
        "\n",
        "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
        "\n",
        "        # Run auxiliary convolutions (higher level feature map generators)\n",
        "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
        "            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
        "\n",
        "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
        "                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        if split in {\"TRAIN\", \"VAL\"}\n",
        "            return locs, classes_scores\n",
        "        else: \n",
        "            return self.detect_objects(locs, classes_scores, self.min_score, self.max_overlap, self.top_k)\n",
        "\n",
        "     def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
        "\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "        predicted_scores = F.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n",
        "\n",
        "        # Lists to store final predicted boxes, labels, and scores for all images\n",
        "        all_images_boxes = list()\n",
        "        all_images_labels = list()\n",
        "        all_images_scores = list()\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Decode object coordinates from the form we regressed predicted boxes to\n",
        "            decoded_locs = cxcy_to_xy(\n",
        "                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  # (8732, 4), these are fractional pt. coordinates\n",
        "\n",
        "            # Lists to store boxes and scores for this image\n",
        "            image_boxes = list()\n",
        "            image_labels = list()\n",
        "            image_scores = list()\n",
        "\n",
        "            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n",
        "\n",
        "            # Check for each class\n",
        "            for c in range(1, self.n_classes):\n",
        "                # Keep only predicted boxes and scores where scores for this class are above the minimum score\n",
        "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
        "                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n",
        "                n_above_min_score = score_above_min_score.sum().item()\n",
        "                if n_above_min_score == 0:\n",
        "                    continue\n",
        "                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n",
        "                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n",
        "\n",
        "                # Sort predicted boxes and scores by scores\n",
        "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
        "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
        "\n",
        "                # Find the overlap between predicted boxes\n",
        "                overlap = find_iou(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score)\n",
        "\n",
        "                # Non-Maximum Suppression (NMS)\n",
        "\n",
        "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).cuda()  # (n_qualified)\n",
        "\n",
        "                # Consider each box in order of decreasing scores\n",
        "                for box in range(class_decoded_locs.size(0)):\n",
        "                    # If this box is already marked for suppression\n",
        "                    if suppress[box] == 1:\n",
        "                        continue\n",
        "\n",
        "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
        "                    # Find such boxes and update suppress indices\n",
        "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
        "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
        "\n",
        "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
        "                    suppress[box] = 0\n",
        "\n",
        "                # Store only unsuppressed boxes for this class\n",
        "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
        "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).cuda())\n",
        "                image_scores.append(class_scores[1 - suppress])\n",
        "\n",
        "            # If no object in any class is found, store a placeholder for 'background'\n",
        "            if len(image_boxes) == 0:\n",
        "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).cuda())\n",
        "                image_labels.append(torch.LongTensor([0]).cuda())\n",
        "                image_scores.append(torch.FloatTensor([0.]).cuda())\n",
        "\n",
        "            # Concatenate into single tensors\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
        "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
        "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
        "            n_objects = image_scores.size(0)\n",
        "\n",
        "            # Keep only the top k objects\n",
        "            if n_objects > top_k:\n",
        "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
        "                image_scores = image_scores[:top_k]  # (top_k)\n",
        "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
        "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
        "\n",
        "            # Append to lists that store predicted boxes and scores for all images\n",
        "            all_images_boxes.append(image_boxes)\n",
        "            all_images_labels.append(image_labels)\n",
        "            all_images_scores.append(image_scores)\n",
        "\n",
        "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esgWuru9MA1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fmap_dims = {'conv4_3': 38,\n",
        "            'conv7': 19,\n",
        "            'conv8_2': 10,\n",
        "            'conv9_2': 5,\n",
        "            'conv10_2': 3,\n",
        "            'conv11_2': 1}\n",
        "\n",
        "obj_scales = {'conv4_3': 0.1,\n",
        "            'conv7': 0.2,\n",
        "            'conv8_2': 0.375,\n",
        "            'conv9_2': 0.55,\n",
        "            'conv10_2': 0.725,\n",
        "            'conv11_2': 0.9}\n",
        "\n",
        "aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
        "                'conv7': [1., 2., 3., 0.5, .333],\n",
        "                'conv8_2': [1., 2., 3., 0.5, .333],\n",
        "                'conv9_2': [1., 2., 3., 0.5, .333],\n",
        "                'conv10_2': [1., 2., 0.5],\n",
        "                'conv11_2': [1., 2., 0.5]}\n",
        "\n",
        "fmaps = list(fmap_dims.keys())\n",
        "\n",
        "BATCH_SZIE = 8\n",
        "EPOCHS = 200\n",
        "WORKERS = 2\n",
        "PRINT_FREQ = 150\n",
        "\n",
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6p6jB6Mb7Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = MyVOCDetection(ROOT_PATH, split=\"TRAIN\", transforms=transform)\n",
        "val_dataset = MyVOCDetection(ROOT_PATH, split=\"VAL\", transforms=transform)\n",
        "\n",
        "train_load = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
        "                        num_workers=2, pin_memory=True, \n",
        "                        collate_fn=train_dataset.collate_fn)\n",
        "val_load = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
        "                        num_workers=2, pin_memory=True, \n",
        "                        collate_fn=val_dataset.collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqxh_SkzOouv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biases = list()\n",
        "not_biases = list()\n",
        "for param_name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        if param_name.endswith('.bias'):\n",
        "            biases.append(param)\n",
        "        else:\n",
        "            not_biases.append(param)\n",
        "optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * LR}, {'params': not_biases, 'lr': LR}],\n",
        "                            momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy5pGO92aYXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(net, dataloader, criterion, optimizer, epoch):\n",
        "\n",
        "    model.train() \n",
        "    \n",
        "    for data in tqdm(dataloader):\n",
        "\n",
        "        images = data[\"img\"].cuda()\n",
        "        boxes = [b.cuda() for b in data[\"boxes\"]]\n",
        "        labels = [l.cuda() for l in data[\"labels\"]]\n",
        "\n",
        "        output = net(images)\n",
        "        \n",
        "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # if grad_clip is not None:\n",
        "        #     clip_gradient(optimizer, grad_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB6Txpdge9ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_epoch(net, train_load, None, None, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIEezCddfoWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}