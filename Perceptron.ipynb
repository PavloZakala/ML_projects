{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZPavlo/ML_projects/blob/master/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umdCSy2Z2ZMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMRREVaVkBG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCH_SIZE = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoBviLuDjxk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_data = np.array([\n",
        "                    [0.0, 0.0],\n",
        "                    [0.0, 1.0],\n",
        "                    [1.0, 0.0],\n",
        "                    [1.0, 1.0],\n",
        "]).astype(\"float32\")\n",
        "true_out_data = np.array([0.0, 1.0, 1.0, 1.0]).astype(\"float32\").T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFGuHMbckqby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "from torch.optim import SGD\n",
        "\n",
        "model = nn.Sequential(nn.Linear(2, 1))\n",
        "model.train()\n",
        "\n",
        "optim = SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHGsQX6P9t0k",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZVTec-Uhjmu",
        "colab_type": "code",
        "outputId": "72c1bdd2-cefb-41f5-8be1-52c75d40e880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "# Perceptron train\n",
        "for ep in range(EPOCH_SIZE):\n",
        "\n",
        "    optim.zero_grad()\n",
        "    data = torch.from_numpy(input_data)\n",
        "    data.requires_grad_(True)\n",
        "    gt_out = torch.from_numpy(true_out_data).view(-1,1)\n",
        "\n",
        "    out = model(data)\n",
        "    loss = ((out - gt_out) ** 2).sum()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if (ep + 1) % 40:\n",
        "        print(\"eposh: {}, loss {}\".format(ep, loss.item()))\n",
        "print(\"Finish\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eposh: 0, loss 12.504171371459961\n",
            "eposh: 1, loss 1.4599257707595825\n",
            "eposh: 2, loss 0.5575854778289795\n",
            "eposh: 3, loss 0.43681639432907104\n",
            "eposh: 4, loss 0.3880161643028259\n",
            "eposh: 5, loss 0.35440966486930847\n",
            "eposh: 6, loss 0.329265832901001\n",
            "eposh: 7, loss 0.310256689786911\n",
            "eposh: 8, loss 0.2958498001098633\n",
            "eposh: 9, loss 0.28491494059562683\n",
            "eposh: 10, loss 0.27660566568374634\n",
            "eposh: 11, loss 0.2702851891517639\n",
            "eposh: 12, loss 0.26547351479530334\n",
            "eposh: 13, loss 0.26180779933929443\n",
            "eposh: 14, loss 0.2590135931968689\n",
            "eposh: 15, loss 0.2568824589252472\n",
            "eposh: 16, loss 0.2552564740180969\n",
            "eposh: 17, loss 0.25401541590690613\n",
            "eposh: 18, loss 0.25306782126426697\n",
            "eposh: 19, loss 0.2523442208766937\n",
            "eposh: 20, loss 0.25179150700569153\n",
            "eposh: 21, loss 0.2513692378997803\n",
            "eposh: 22, loss 0.2510465681552887\n",
            "eposh: 23, loss 0.2507999837398529\n",
            "eposh: 24, loss 0.2506115734577179\n",
            "eposh: 25, loss 0.25046756863594055\n",
            "eposh: 26, loss 0.25035738945007324\n",
            "eposh: 27, loss 0.2502732574939728\n",
            "eposh: 28, loss 0.2502089738845825\n",
            "eposh: 29, loss 0.25015977025032043\n",
            "eposh: 30, loss 0.2501221299171448\n",
            "eposh: 31, loss 0.25009340047836304\n",
            "eposh: 32, loss 0.2500714063644409\n",
            "eposh: 33, loss 0.25005465745925903\n",
            "eposh: 34, loss 0.25004178285598755\n",
            "eposh: 35, loss 0.2500319182872772\n",
            "eposh: 36, loss 0.25002434849739075\n",
            "eposh: 37, loss 0.2500186562538147\n",
            "eposh: 38, loss 0.2500142753124237\n",
            "eposh: 40, loss 0.25000831484794617\n",
            "eposh: 41, loss 0.2500063478946686\n",
            "eposh: 42, loss 0.2500048875808716\n",
            "eposh: 43, loss 0.2500036954879761\n",
            "eposh: 44, loss 0.25000283122062683\n",
            "eposh: 45, loss 0.2500022053718567\n",
            "eposh: 46, loss 0.2500016689300537\n",
            "eposh: 47, loss 0.2500012516975403\n",
            "eposh: 48, loss 0.2500009834766388\n",
            "eposh: 49, loss 0.2500007152557373\n",
            "eposh: 50, loss 0.25000056624412537\n",
            "eposh: 51, loss 0.2500004172325134\n",
            "eposh: 52, loss 0.25000035762786865\n",
            "eposh: 53, loss 0.2500002682209015\n",
            "eposh: 54, loss 0.2500002086162567\n",
            "eposh: 55, loss 0.2500001788139343\n",
            "eposh: 56, loss 0.25000014901161194\n",
            "eposh: 57, loss 0.2500000596046448\n",
            "eposh: 58, loss 0.2500000596046448\n",
            "eposh: 59, loss 0.2500000596046448\n",
            "eposh: 60, loss 0.25\n",
            "eposh: 61, loss 0.25\n",
            "eposh: 62, loss 0.2500000298023224\n",
            "eposh: 63, loss 0.2500000596046448\n",
            "eposh: 64, loss 0.2499999701976776\n",
            "eposh: 65, loss 0.25\n",
            "eposh: 66, loss 0.25\n",
            "eposh: 67, loss 0.2499999701976776\n",
            "eposh: 68, loss 0.24999994039535522\n",
            "eposh: 69, loss 0.25\n",
            "eposh: 70, loss 0.25\n",
            "eposh: 71, loss 0.2499999850988388\n",
            "eposh: 72, loss 0.25\n",
            "eposh: 73, loss 0.25\n",
            "eposh: 74, loss 0.2499999701976776\n",
            "eposh: 75, loss 0.2499999701976776\n",
            "eposh: 76, loss 0.25\n",
            "eposh: 77, loss 0.2499999701976776\n",
            "eposh: 78, loss 0.25\n",
            "eposh: 80, loss 0.25\n",
            "eposh: 81, loss 0.25\n",
            "eposh: 82, loss 0.2499999701976776\n",
            "eposh: 83, loss 0.2499999701976776\n",
            "eposh: 84, loss 0.2500000298023224\n",
            "eposh: 85, loss 0.25\n",
            "eposh: 86, loss 0.25\n",
            "eposh: 87, loss 0.2499999850988388\n",
            "eposh: 88, loss 0.2500000298023224\n",
            "eposh: 89, loss 0.2500000298023224\n",
            "eposh: 90, loss 0.2499999850988388\n",
            "eposh: 91, loss 0.25\n",
            "eposh: 92, loss 0.24999995529651642\n",
            "eposh: 93, loss 0.25\n",
            "eposh: 94, loss 0.25\n",
            "eposh: 95, loss 0.2500000298023224\n",
            "eposh: 96, loss 0.2499999850988388\n",
            "eposh: 97, loss 0.25\n",
            "eposh: 98, loss 0.2499999701976776\n",
            "eposh: 99, loss 0.24999994039535522\n",
            "eposh: 100, loss 0.2500000298023224\n",
            "eposh: 101, loss 0.2499999701976776\n",
            "eposh: 102, loss 0.2499999850988388\n",
            "eposh: 103, loss 0.2500000298023224\n",
            "eposh: 104, loss 0.2500000298023224\n",
            "eposh: 105, loss 0.2499999701976776\n",
            "eposh: 106, loss 0.2500000596046448\n",
            "eposh: 107, loss 0.2499999701976776\n",
            "eposh: 108, loss 0.25\n",
            "eposh: 109, loss 0.2499999850988388\n",
            "eposh: 110, loss 0.2499999850988388\n",
            "eposh: 111, loss 0.2500000298023224\n",
            "eposh: 112, loss 0.2500000298023224\n",
            "eposh: 113, loss 0.25\n",
            "eposh: 114, loss 0.2500000596046448\n",
            "eposh: 115, loss 0.2499999850988388\n",
            "eposh: 116, loss 0.2500000596046448\n",
            "eposh: 117, loss 0.2500000298023224\n",
            "eposh: 118, loss 0.2500000298023224\n",
            "eposh: 120, loss 0.2500000298023224\n",
            "eposh: 121, loss 0.2500000298023224\n",
            "eposh: 122, loss 0.2500000298023224\n",
            "eposh: 123, loss 0.2500000298023224\n",
            "eposh: 124, loss 0.2500000298023224\n",
            "eposh: 125, loss 0.2500000298023224\n",
            "eposh: 126, loss 0.2500000298023224\n",
            "eposh: 127, loss 0.2500000298023224\n",
            "eposh: 128, loss 0.2500000298023224\n",
            "eposh: 129, loss 0.2500000298023224\n",
            "eposh: 130, loss 0.2500000298023224\n",
            "eposh: 131, loss 0.2500000298023224\n",
            "eposh: 132, loss 0.2500000298023224\n",
            "eposh: 133, loss 0.2500000298023224\n",
            "eposh: 134, loss 0.2500000298023224\n",
            "eposh: 135, loss 0.2500000298023224\n",
            "eposh: 136, loss 0.2500000298023224\n",
            "eposh: 137, loss 0.2500000298023224\n",
            "eposh: 138, loss 0.2500000298023224\n",
            "eposh: 139, loss 0.2500000298023224\n",
            "eposh: 140, loss 0.2500000298023224\n",
            "eposh: 141, loss 0.2500000298023224\n",
            "eposh: 142, loss 0.2500000298023224\n",
            "eposh: 143, loss 0.2500000298023224\n",
            "eposh: 144, loss 0.2500000298023224\n",
            "eposh: 145, loss 0.2500000298023224\n",
            "eposh: 146, loss 0.2500000298023224\n",
            "eposh: 147, loss 0.2500000298023224\n",
            "eposh: 148, loss 0.2500000298023224\n",
            "eposh: 149, loss 0.2500000298023224\n",
            "eposh: 150, loss 0.2500000298023224\n",
            "eposh: 151, loss 0.2500000298023224\n",
            "eposh: 152, loss 0.2500000298023224\n",
            "eposh: 153, loss 0.2500000298023224\n",
            "eposh: 154, loss 0.2500000298023224\n",
            "eposh: 155, loss 0.2500000298023224\n",
            "eposh: 156, loss 0.2500000298023224\n",
            "eposh: 157, loss 0.2500000298023224\n",
            "eposh: 158, loss 0.2500000298023224\n",
            "eposh: 160, loss 0.2500000298023224\n",
            "eposh: 161, loss 0.2500000298023224\n",
            "eposh: 162, loss 0.2500000298023224\n",
            "eposh: 163, loss 0.2500000298023224\n",
            "eposh: 164, loss 0.2500000298023224\n",
            "eposh: 165, loss 0.2500000298023224\n",
            "eposh: 166, loss 0.2500000298023224\n",
            "eposh: 167, loss 0.2500000298023224\n",
            "eposh: 168, loss 0.2500000298023224\n",
            "eposh: 169, loss 0.2500000298023224\n",
            "eposh: 170, loss 0.2500000298023224\n",
            "eposh: 171, loss 0.2500000298023224\n",
            "eposh: 172, loss 0.2500000298023224\n",
            "eposh: 173, loss 0.2500000298023224\n",
            "eposh: 174, loss 0.2500000298023224\n",
            "eposh: 175, loss 0.2500000298023224\n",
            "eposh: 176, loss 0.2500000298023224\n",
            "eposh: 177, loss 0.2500000298023224\n",
            "eposh: 178, loss 0.2500000298023224\n",
            "eposh: 179, loss 0.2500000298023224\n",
            "eposh: 180, loss 0.2500000298023224\n",
            "eposh: 181, loss 0.2500000298023224\n",
            "eposh: 182, loss 0.2500000298023224\n",
            "eposh: 183, loss 0.2500000298023224\n",
            "eposh: 184, loss 0.2500000298023224\n",
            "eposh: 185, loss 0.2500000298023224\n",
            "eposh: 186, loss 0.2500000298023224\n",
            "eposh: 187, loss 0.2500000298023224\n",
            "eposh: 188, loss 0.2500000298023224\n",
            "eposh: 189, loss 0.2500000298023224\n",
            "eposh: 190, loss 0.2500000298023224\n",
            "eposh: 191, loss 0.2500000298023224\n",
            "eposh: 192, loss 0.2500000298023224\n",
            "eposh: 193, loss 0.2500000298023224\n",
            "eposh: 194, loss 0.2500000298023224\n",
            "eposh: 195, loss 0.2500000298023224\n",
            "eposh: 196, loss 0.2500000298023224\n",
            "eposh: 197, loss 0.2500000298023224\n",
            "eposh: 198, loss 0.2500000298023224\n",
            "Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i88aDGTi6Yh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "96b063b0-7e40-4602-8f23-9626bb64d2da"
      },
      "source": [
        "test_data = torch.from_numpy(input_data)\n",
        "for d in test_data:\n",
        "    out = int(model(d.unsqueeze(0)).item() > 0.5)\n",
        "    d = [int(p) for p in d]\n",
        "    print(\"input {} : output {}\".format(d, out))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input [0, 0] : output 0\n",
            "input [0, 1] : output 1\n",
            "input [1, 0] : output 1\n",
            "input [1, 1] : output 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}